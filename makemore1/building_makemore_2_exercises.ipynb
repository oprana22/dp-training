{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "E01: train a trigram language model"
      ],
      "metadata": {
        "id": "i2K1jI6kIjRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "\n",
        "with open(\"names.txt\", \"r\") as f:\n",
        "  words = f.read().splitlines()\n",
        "  # print(words[:5], len(words), sep = \"\\n\")"
      ],
      "metadata": {
        "id": "ny1tAhOkKhpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dictionary to transform bigrams to int\n",
        "#dot encodes for end and beggining.\n",
        "from collections import defaultdict\n",
        "bi_to_num = defaultdict()\n",
        "s = \".abcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "count = 0\n",
        "for char in s:\n",
        "  for char2 in s:\n",
        "    bi_to_num[char+char2] = count\n",
        "    count += 1\n",
        "# print(bi_to_num.items())\n",
        "# print(bi_to_num[\"an\"])\n",
        "# len(bi_to_num)"
      ],
      "metadata": {
        "id": "_faXthY3yQCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dictionary to transform char to int\n",
        "num_to_char = defaultdict() #should be called char_to_num\n",
        "\n",
        "count = 0\n",
        "for char in s:\n",
        "  num_to_char[char] = count\n",
        "  count += 1\n",
        "# print(num_to_char.items())\n",
        "# print(num_to_char[\"a\"])\n",
        "# len(num_to_char)\n",
        "\n",
        "#dict to transform int to char\n",
        "int_to_char = {v: k for k, v in num_to_char.items()}"
      ],
      "metadata": {
        "id": "A9_T9XKWi8Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build dataset of xs and ys (inputs and labels)\n",
        "xs, ys = [], []\n",
        "\n",
        "for w in words:\n",
        "  wls = [\".\"] + list(w) + [\".\"] #dots indicate ending and beggining\n",
        "  bi_ls = list() #temporary list used to build xs and ys\n",
        "\n",
        "  for char1, char2 in zip(wls, wls[1:]):\n",
        "    bi_ls.append(char1+char2)\n",
        "  # print(bi_ls)\n",
        "\n",
        "  for nninput, label in zip(bi_ls, wls[2:]):\n",
        "    xs.append(bi_to_num[nninput]); ys.append(num_to_char[label])\n",
        "  # print(xs, ys)\n",
        "  del bi_ls\n",
        "\n",
        "# print(xs[:5])\n"
      ],
      "metadata": {
        "id": "t0_pZet1aEOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = xs.nelement()\n",
        "print('number of examples: ', num)\n",
        "# print(xs.dtype)\n"
      ],
      "metadata": {
        "id": "iyvWaebzDkjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb8d87a-c228-4780-8f1b-d3c9f246c1aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of examples:  196113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encode inputs to feed nn\n",
        "xs_enc = torch.nn.functional.one_hot(xs, num_classes=729).float()\n",
        "# print(xs_enc.shape, xs_enc.dtype)\n",
        "\n",
        "#set seed for reproducibility\n",
        "g_cpu = torch.Generator()\n",
        "g_cpu.manual_seed(2147483641)\n",
        "\n",
        "#create W\n",
        "W = torch.randn([729, 27], generator=g_cpu, requires_grad=True)\n",
        "# print(W[0,0].item())"
      ],
      "metadata": {
        "id": "Sqp-s0SDGBHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(100):\n",
        "\n",
        "  ## FORWARD PASS\n",
        "  logits = xs_enc @ W\n",
        "  # softmax\n",
        "  counts = torch.exp(logits)\n",
        "  prob = counts / torch.sum(counts, 1, keepdim = True)\n",
        "\n",
        "  # loss function - averaged negative log likelyhood\n",
        "  # loss = -torch.sum(torch.log(prob), 1, keepdim = True)\n",
        "  # loss = torch.mean(loss)\n",
        "  loss = -prob[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
        "  print(loss.item())\n",
        "  # loss is just one number, a scalar ( the average )\n",
        "\n",
        "  ## BACKPROPAGATION\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # print(W.grad[5])\n",
        "  # print(W.data[5])\n",
        "\n",
        "  ## TWEAK WEIGHTS BASED ON GRADIENT\n",
        "\n",
        "  W.data += 50 * -W.grad\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbhE_eyjd3ls",
        "outputId": "e24f5f39-b8b3-491f-add8-5a4dd118aadb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.835066318511963\n",
            "3.753251791000366\n",
            "3.6761972904205322\n",
            "3.6037278175354004\n",
            "3.5357553958892822\n",
            "3.4722180366516113\n",
            "3.4130465984344482\n",
            "3.3581457138061523\n",
            "3.3073959350585938\n",
            "3.260652542114258\n",
            "3.2177305221557617\n",
            "3.1783885955810547\n",
            "3.1423232555389404\n",
            "3.109186887741089\n",
            "3.0786244869232178\n",
            "3.050307512283325\n",
            "3.0239510536193848\n",
            "2.999321222305298\n",
            "2.9762251377105713\n",
            "2.954505681991577\n",
            "2.934030771255493\n",
            "2.9146907329559326\n",
            "2.8963890075683594\n",
            "2.879041910171509\n",
            "2.8625741004943848\n",
            "2.8469178676605225\n",
            "2.8320116996765137\n",
            "2.8177993297576904\n",
            "2.804229497909546\n",
            "2.7912542819976807\n",
            "2.7788314819335938\n",
            "2.766921043395996\n",
            "2.755488157272339\n",
            "2.7444989681243896\n",
            "2.7339248657226562\n",
            "2.7237374782562256\n",
            "2.713914394378662\n",
            "2.7044317722320557\n",
            "2.6952693462371826\n",
            "2.6864089965820312\n",
            "2.677833080291748\n",
            "2.6695263385772705\n",
            "2.6614744663238525\n",
            "2.6536641120910645\n",
            "2.646082639694214\n",
            "2.638718605041504\n",
            "2.6315624713897705\n",
            "2.624603509902954\n",
            "2.6178324222564697\n",
            "2.611241340637207\n",
            "2.6048219203948975\n",
            "2.598567247390747\n",
            "2.5924699306488037\n",
            "2.5865237712860107\n",
            "2.5807223320007324\n",
            "2.5750603675842285\n",
            "2.5695316791534424\n",
            "2.5641326904296875\n",
            "2.558856964111328\n",
            "2.5537009239196777\n",
            "2.5486605167388916\n",
            "2.5437309741973877\n",
            "2.5389089584350586\n",
            "2.5341906547546387\n",
            "2.5295727252960205\n",
            "2.5250518321990967\n",
            "2.520624876022339\n",
            "2.516288995742798\n",
            "2.5120410919189453\n",
            "2.507878303527832\n",
            "2.503798484802246\n",
            "2.4997990131378174\n",
            "2.495877265930176\n",
            "2.4920310974121094\n",
            "2.4882586002349854\n",
            "2.4845571517944336\n",
            "2.4809248447418213\n",
            "2.4773597717285156\n",
            "2.473860263824463\n",
            "2.470424175262451\n",
            "2.467050075531006\n",
            "2.463736057281494\n",
            "2.4604804515838623\n",
            "2.457282066345215\n",
            "2.4541385173797607\n",
            "2.451049327850342\n",
            "2.448012590408325\n",
            "2.4450271129608154\n",
            "2.442091703414917\n",
            "2.439204692840576\n",
            "2.4363653659820557\n",
            "2.4335718154907227\n",
            "2.430823802947998\n",
            "2.428119421005249\n",
            "2.4254579544067383\n",
            "2.4228384494781494\n",
            "2.420259714126587\n",
            "2.4177207946777344\n",
            "2.4152207374572754\n",
            "2.4127590656280518\n",
            "2.410334348678589\n",
            "2.407945394515991\n",
            "2.405592441558838\n",
            "2.4032740592956543\n",
            "2.400989532470703\n",
            "2.398737907409668\n",
            "2.3965187072753906\n",
            "2.394331455230713\n",
            "2.39217472076416\n",
            "2.3900485038757324\n",
            "2.3879520893096924\n",
            "2.3858845233917236\n",
            "2.383845567703247\n",
            "2.381834030151367\n",
            "2.379850149154663\n",
            "2.37789249420166\n",
            "2.3759610652923584\n",
            "2.3740553855895996\n",
            "2.3721745014190674\n",
            "2.3703181743621826\n",
            "2.368485927581787\n",
            "2.3666772842407227\n",
            "2.36489200592041\n",
            "2.363129138946533\n",
            "2.3613884449005127\n",
            "2.3596699237823486\n",
            "2.3579723834991455\n",
            "2.3562958240509033\n",
            "2.354639768600464\n",
            "2.353003978729248\n",
            "2.3513882160186768\n",
            "2.3497917652130127\n",
            "2.3482143878936768\n",
            "2.3466556072235107\n",
            "2.3451151847839355\n",
            "2.343593120574951\n",
            "2.3420889377593994\n",
            "2.3406014442443848\n",
            "2.3391313552856445\n",
            "2.3376784324645996\n",
            "2.3362417221069336\n",
            "2.3348214626312256\n",
            "2.3334169387817383\n",
            "2.332028388977051\n",
            "2.330655097961426\n",
            "2.329296827316284\n",
            "2.327953577041626\n",
            "2.326625108718872\n",
            "2.3253111839294434\n",
            "2.3240110874176025\n",
            "2.3227250576019287\n",
            "2.3214528560638428\n",
            "2.3201940059661865\n",
            "2.318948268890381\n",
            "2.317716598510742\n",
            "2.3164968490600586\n",
            "2.3152902126312256\n",
            "2.3140957355499268\n",
            "2.3129138946533203\n",
            "2.311743974685669\n",
            "2.3105859756469727\n",
            "2.3094394207000732\n",
            "2.308305263519287\n",
            "2.3071818351745605\n",
            "2.306069850921631\n",
            "2.3049685955047607\n",
            "2.3038787841796875\n",
            "2.3027992248535156\n",
            "2.3017303943634033\n",
            "2.3006722927093506\n",
            "2.29962420463562\n",
            "2.298586368560791\n",
            "2.297558069229126\n",
            "2.2965402603149414\n",
            "2.295531749725342\n",
            "2.2945327758789062\n",
            "2.293543577194214\n",
            "2.2925636768341064\n",
            "2.291592597961426\n",
            "2.29063081741333\n",
            "2.2896780967712402\n",
            "2.288734197616577\n",
            "2.2877986431121826\n",
            "2.286871910095215\n",
            "2.2859535217285156\n",
            "2.285043478012085\n",
            "2.284142017364502\n",
            "2.2832484245300293\n",
            "2.282362699508667\n",
            "2.281484842300415\n",
            "2.2806146144866943\n",
            "2.2797529697418213\n",
            "2.278898239135742\n",
            "2.2780511379241943\n",
            "2.2772114276885986\n",
            "2.276379108428955\n",
            "2.2755539417266846\n",
            "2.274736166000366\n",
            "2.2739250659942627\n",
            "2.2731211185455322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sample to create names:\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# finally, sample from the 'neural net' model\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  out = []\n",
        "  ix = 0\n",
        "  while True:\n",
        "\n",
        "    # ----------\n",
        "    # BEFORE:\n",
        "    #p = P[ix]\n",
        "    # ----------\n",
        "    # NOW:\n",
        "   ## FORWARD PASS\n",
        "    xs_enc = F.one_hot(torch.tensor([ix]), num_classes=729).float()\n",
        "\n",
        "    logits = xs_enc @ W\n",
        "    # softmax\n",
        "    counts = torch.exp(logits)\n",
        "    p = counts / torch.sum(counts, 1, keepdim = True)\n",
        "    # ----------\n",
        "\n",
        "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(int_to_char[ix])\n",
        "    if ix == 0:\n",
        "      break\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5G7_tAA5eqc",
        "outputId": "926ab435-46ce-4021-9ea8-6221ad850eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "junidedilicaazoprofayolinareshinitolemamarezavilanaauzamileviaskabdainrwimelesezyanayvarolofavaumaryfolemajanonasueloruanirhorelyarhocaradaabrinedemikimayfineviaasnamalimopazahadiosfanadavilu.\n",
            "repanamisharanareli.\n",
            "isamujerumjemujemaluunwilapvavicoarorydahuedartanapayosbrevinoiqululoocalihemimayawathf.\n",
            ".\n",
            "nelalopvilitikemialoevevinanolokalelenemadanolaryawimuralodridrozelavalilpargraitezraleliquvamalelesemosahabrhauelazmiminosanoyoinalixalipremenilocadaffadenelavjankesoranajedayaraanolyabrelihanazyodabrucuolobasatelyxilabujadasavenjayalyudfrequladoshatalalokamalemadmanistfalamadedihavigaisaresyqivixi.\n"
          ]
        }
      ]
    }
  ]
}